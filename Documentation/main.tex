\documentclass{article}
\usepackage{graphicx,datetime,hyperref} % Required for inserting images

\usepackage{geometry}
\geometry{
a4paper,
total={170mm,257mm},
left=20mm,
top=20mm,
}

\newcommand{\setlasteditor}[1]{\gdef\lasteditor{#1}}
\newcommand{\lastedited}{%
    \vspace{1mm} {\footnotesize Last edited by: \lasteditor} \vspace{3mm}
    \newline

}

\title{ISTA ASTRO SOFTWARE}
\date{September 2024}

\begin{document}

\maketitle


\tableofcontents
\newpage
\section*{Introduction}
\setlasteditor{Aayush Desai}
\lastedited
\noindent
Hello world. This is the ISTA Software Page, which will hopefully have all the neat tools and tricks the department would have developed over the years. The repository is maintained by Aayush Desai for the time being, and is open to contributions from anyone who would like to add to it.
\newline
\noindent
Here you can find access to example scripts that are group agnostic (mostly). Should you have any ideas for further developments, just add another section and start writing.
\newline
\newline
\textbf{For those working on Overleaf, once you are done with the edits, please make sure to push the changes to the repository. You can do this by going to menu on the top left corner, and selecting Github. From there, you can commit the changes and push them to the repository.When starting afresh, please remember to go to Menu in the top right, under sync go to github, and pull from it, to avoid merge conflicts.} 
\newline
\newline
\textbf{For those working on the local machine, please make sure to push the changes to the repository.}
\section{Sharing Repositories via GIT}
\setlasteditor{Aayush Desai}
\lastedited

\section{MESA on Cluster}
\setlasteditor{Lukas Einramhof}
\lastedited
IT has installed \texttt{MESA} on the cluster. In order to get \texttt{MESA} running on the cluster, a few steps have to be done to ensure \texttt{MESA} is running correctly.

Two templates \texttt{slurm} scripts (one for a normal job and one for an array job) are available in this repository under \texttt{Templates} (\texttt{run\_mesa.sh} and \texttt{run\_mesa\_array.sh}). \\

First the (current version of the) \texttt{MESA} module has to be loaded via
\begin{verbatim}
    module load mesastar/23.05.1
\end{verbatim}
where `23.05.1' is the current (and only) version of \texttt{MESA} installed on the cluster. 
If other versions are needed you need to write an email to IT with the specific version that they should install.

To get a template directory for your \texttt{MESA} run (if you don't have one yourself) you can copy the \texttt{\$MESA\_DIR/star/work/} directory into your cluster directory.
Note that the \texttt{\$MESA\_DIR} command is only available after loading the \texttt{MESA} module.

One big caveate with \texttt{MESA} installed on the cluster is, that the default cache folders in the \texttt{\$MESA\_DIR} can't be written to.
Thus, for every run you have to define a custom cache folder in your personal cluster folder. Furthermore, each \texttt{MESA} run has to have its own cache folder. This is especially important if you run array jobs.
To set up a custom cache you will have to run
\begin{verbatim}
    mkdir path_to_custom_cache_dir
    export MESA_CACHES_DIR=path_to_custom_cache_dir
\end{verbatim}
The first command creates a custom cache directory at some specified path, and the second command then sets the \texttt{MESA\_CACHES\_DIR} flag to the corresponding directory which tells \texttt{MESA} where to put the cache files.
Since each \texttt{MESA} run needs its own cache directory, I would suggest putting the caches folder directly into the directory where the specific \texttt{MESA} run is setup. For example create a folder structure like
\begin{verbatim}
    work/
    |
    +-- make/
    +-- src/
    +-- caches/
    +-- clean
    +-- mk
    +-- rn
    +-- ...
\end{verbatim}
and then add the command
\begin{verbatim}
    export MESA_CACHES_DIR=work_dir/caches
\end{verbatim}
into your \texttt{slurm} script.

With all this setup, there should be no issues in running \texttt{MESA} on the cluster.



\section{Tips for working on the Cluster}
\setlasteditor{Lukas Einramhof}
\lastedited

\subsection{Making your setup more efficient to get time on the cluster faster}
\setlasteditor{Lukas Einramhof}
\lastedited
\noindent
After you have run a job for the first time you can check how much CPU and memory it used to then tailor your next run to use just enough resources. This will let your next job run earlier since it uses less resources.
To do that run
\begin{verbatim}
    module load seff
    seff JOB_ID
\end{verbatim}
This will give you output similar to
\begin{verbatim}
    ...
    Job ID: 23693144
    Array Job ID: 23693144_9
    Cluster: istscicomp
    User/Group: leinramh/bugnegrp
    State: COMPLETED (exit code 0)
    Nodes: 1
    Cores per node: 4
    CPU Utilized: 12:13:44
--> CPU Efficiency: 95.75% of 12:46:16 core-walltime
    Job Wall-clock time: 03:11:34
    Memory Utilized: 15.75 GB
--> Memory Efficiency: 49.21% of 32.00 GB
\end{verbatim}
With the \texttt{CPU Efficiency} and the \texttt{Memory Efficiency} you can adjust how much memory and cpus you ask for in the next run, to get faster priority (unless you need more resources of course).
In the example above I could have asked for half the amount of memory.

\subsection{Running on the head vs. submitting via slurm scripts}
\setlasteditor{Aayush Desai}
\lastedited
\noindent
As a general rule of thumb, it is always better to submit jobs via slurm scripts, rather than running them on the head node. This is because the head node is a shared resource, and running jobs on it can slow down the system for everyone else. Furthermore, running jobs on the head node can lead to the job being killed if it uses too many resources or whenever you logout. 
\newline
\newline
There are really helpful slurm scripts on the IT page, which can be found at: \href{https://it.pages.ist.ac.at/docs/hpc-cluster/hpc-handbook/composing-a-slurm-script/}{here}. These scripts can be used to submit jobs to the cluster, and can be modified to suit your needs.
\newpage
Jobs that could be run on the head node:

\begin{enumerate}
    \item Small jobs that require less than 5 minutes to run.
    \item Jobs that require less than 1GB of memory.
    \item Jobs that require less than 1 core.
    \item File IO operations: eg copying files, moving files, etc\footnote{If you are running such a job on the head node, and you would like to move it to the cluster, you can use the \texttt{screen} command. This command allows you to run a job in the background, and can be detached from the terminal. This way, you can run the job on the head node, and then detach it and move it to the cluster. Please do not misuse this to run bigger jobs on the headnode. Lets try to respect the \textit{shared} resource}.
\end{enumerate}






    

\section{Working on GIT}


\end{document}
